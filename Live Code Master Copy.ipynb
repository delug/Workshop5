{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets training and testing data for MNIST dataset\n",
    "trainData = datasets.MNIST('data/', train = True, transform = transforms.ToTensor(), download=True)\n",
    "testData = datasets.MNIST('data/', train = False, transform = transforms.ToTensor(), download=True)\n",
    "\n",
    "#constructs loaders from datasets\n",
    "trainLoader = torch.utils.data.DataLoader(trainData, batch_size=100, shuffle=True)\n",
    "testLoader = torch.utils.data.DataLoader(testData, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convolutional layers are instantiated with Conv2d:\n",
    "self.newLayer = nn.Conv2d(input_channels, output_channels, filter_size)\n",
    "'''\n",
    "\n",
    "'''\n",
    "First Convolutional Neural Network\n",
    "'''\n",
    "class ConvNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet1, self).__init__()\n",
    "        #(1,28,28) -> (8,22,22)\n",
    "        self.conv1 = nn.Conv2d(1, 8, 7)\n",
    "        \n",
    "        #(8,22,22) -> (16,17,17)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 6)\n",
    "        \n",
    "        #(16,17,17) -> (32,12,12)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 6)\n",
    "        \n",
    "        #(32,12,12) -> (64,8,8)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 5)\n",
    "\n",
    "        #(64,8,8) -> (80,4,4)\n",
    "        self.conv5 = nn.Conv2d(64, 80, 5)\n",
    "        \n",
    "        #(80,4,4) -> (100,2,2)\n",
    "        self.conv6 = nn.Conv2d(80, 100, 3)\n",
    "        \n",
    "        #400 -> 10\n",
    "        self.linear = nn.Linear(400, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #(1,28,28) -> (8,22,22)\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        \n",
    "        #(8,22,22) -> (16,17,17)\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        \n",
    "        #(16,17,17) -> (32,12,12)\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        \n",
    "        #(32,12,12) -> (64,8,8)\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        \n",
    "        #(64,8,8) -> (80,4,4)\n",
    "        x = torch.sigmoid(self.conv5(x))\n",
    "        \n",
    "        #(80,4,4) -> (100,2,2)\n",
    "        x = torch.sigmoid(self.conv6(x))\n",
    "        \n",
    "        #(100,2,2) -> 400\n",
    "        x = x.view(-1, 400)\n",
    "        \n",
    "        #400 -> 10\n",
    "        x = F.softmax(self.linear(x), dim=1)\n",
    "        \n",
    "        return x        \n",
    "    \n",
    "model = ConvNet1()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    trainingLoss = 0\n",
    "    for index, (data, target) in enumerate(trainLoader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(data)\n",
    "        batchLoss = F.cross_entropy(predictions, target)\n",
    "        batchLoss.backward()\n",
    "        trainingLoss += batchLoss.item()\n",
    "        optimizer.step()\n",
    "        if index % 10 == 0:\n",
    "            print(f'Training Epoch: {epoch} [{index * len(data)} / {len(trainData)}]\\tLoss: {(batchLoss.item() / len(data)):.6f}')\n",
    "    print(f'----- Epoch: {epoch} Average loss: {(trainingLoss/len(trainData)):.4f}')\n",
    "    \n",
    "def test():\n",
    "    testingLoss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for index, (data, target) in enumerate(testLoader):\n",
    "            predictions = model(data)\n",
    "            testingLoss += F.cross_entropy(predictions, target).item()\n",
    "            _, predictedValues = torch.max(predictions, 1)\n",
    "            print('target is: ')\n",
    "            print(target)\n",
    "            print('predicted values are: ')\n",
    "            print(predictedValues)\n",
    "            total += target.size(0)\n",
    "            correct += (predictedValues == target).sum().item()\n",
    "    print(f'----- Test set loss: {(testingLoss):.4f}')\n",
    "    print(f'----- Accuracy: {correct/total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [0 / 60000]\tLoss: 0.023072\n",
      "Training Epoch: 1 [1000 / 60000]\tLoss: 0.023036\n",
      "Training Epoch: 1 [2000 / 60000]\tLoss: 0.022984\n",
      "Training Epoch: 1 [3000 / 60000]\tLoss: 0.023044\n",
      "Training Epoch: 1 [4000 / 60000]\tLoss: 0.023067\n",
      "Training Epoch: 1 [5000 / 60000]\tLoss: 0.022969\n",
      "Training Epoch: 1 [6000 / 60000]\tLoss: 0.023102\n",
      "Training Epoch: 1 [7000 / 60000]\tLoss: 0.022996\n",
      "Training Epoch: 1 [8000 / 60000]\tLoss: 0.023058\n",
      "Training Epoch: 1 [9000 / 60000]\tLoss: 0.022964\n",
      "Training Epoch: 1 [10000 / 60000]\tLoss: 0.023067\n",
      "Training Epoch: 1 [11000 / 60000]\tLoss: 0.022962\n",
      "Training Epoch: 1 [12000 / 60000]\tLoss: 0.023026\n",
      "Training Epoch: 1 [13000 / 60000]\tLoss: 0.022774\n",
      "Training Epoch: 1 [14000 / 60000]\tLoss: 0.022570\n",
      "Training Epoch: 1 [15000 / 60000]\tLoss: 0.022644\n",
      "Training Epoch: 1 [16000 / 60000]\tLoss: 0.022271\n",
      "Training Epoch: 1 [17000 / 60000]\tLoss: 0.022219\n",
      "Training Epoch: 1 [18000 / 60000]\tLoss: 0.022322\n",
      "Training Epoch: 1 [19000 / 60000]\tLoss: 0.022071\n",
      "Training Epoch: 1 [20000 / 60000]\tLoss: 0.021707\n",
      "Training Epoch: 1 [21000 / 60000]\tLoss: 0.022035\n",
      "Training Epoch: 1 [22000 / 60000]\tLoss: 0.022464\n",
      "Training Epoch: 1 [23000 / 60000]\tLoss: 0.022097\n",
      "Training Epoch: 1 [24000 / 60000]\tLoss: 0.021863\n",
      "Training Epoch: 1 [25000 / 60000]\tLoss: 0.022387\n",
      "Training Epoch: 1 [26000 / 60000]\tLoss: 0.021617\n",
      "Training Epoch: 1 [27000 / 60000]\tLoss: 0.021870\n",
      "Training Epoch: 1 [28000 / 60000]\tLoss: 0.021757\n",
      "Training Epoch: 1 [29000 / 60000]\tLoss: 0.021580\n",
      "Training Epoch: 1 [30000 / 60000]\tLoss: 0.021503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-762205dac794>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-1436dd74c261>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mbatchLoss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mbatchLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mtrainingLoss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatchLoss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\qwyne\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\qwyne\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1,11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Second Convolutional Neural Network\n",
    "'''\n",
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet2, self).__init__()\n",
    "        #(1,28,28) -> (16,24,22)\n",
    "        self.conv1 = nn.Conv2d(1, 16, (5,7))\n",
    "        \n",
    "        #(16,24,22) -> (32,21,19)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 8, padding=2)\n",
    "        \n",
    "        #(32,21,19) -> (64,10,9)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, stride=2)\n",
    "        \n",
    "        #(64,10,9) -> (128,6,6)\n",
    "        self.conv4 = nn.Conv2d(64, 128, (5,6), padding=(0,1))\n",
    "        \n",
    "        #(128,6,6) -> (256,2,2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, 5)\n",
    "        \n",
    "        #1024 -> 10\n",
    "        self.linear = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #(1,28,28) -> (16,24,22)\n",
    "        x = torch.sigmoid(self.conv1(x))  \n",
    "        \n",
    "        #(16,24,22) -> (32,21,19)\n",
    "        x = torch.sigmoid(self.conv2(x))  \n",
    "        \n",
    "        #(32,21,19) -> (64,10,9)\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        \n",
    "        #(64,10,9) -> (128,6,6)\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        \n",
    "        #(128,6,6) -> (256,2,2)\n",
    "        x = torch.sigmoid(self.conv5(x))\n",
    "        \n",
    "        #(256,2,2) -> 1024\n",
    "        x = x.view(-1, 1024)\n",
    "        \n",
    "        #1024 -> 10\n",
    "        x = F.softmax(self.linear(x), dim=1)\n",
    "        \n",
    "        return x        \n",
    "    \n",
    "model = ConvNet2()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Exercise: Make a Convolutional Neural Network meeting the following specifications:\n",
    "1. Apply a convolutional layer with a padding and stride of 1 that takes in one input channel\n",
    "and produces 14 output channels of size 22 x 25.\n",
    "2. Apply a convolutional layer with a kernel size of 5 x 9 that produces 20 output channels of size 8 x 7\n",
    "3. Apply a convolutional layer with a kernel size of 5 x 7 that produces 25 output channels of size 8 x 7\n",
    "4. Apply any number of convolutional layers to yield an output of 30 channels of size 4 x 4\n",
    "5. Flatten the output\n",
    "6. Apply two linear layers to transform the output to a vector of length 10. Apply softmax and return.\n",
    "'''\n",
    "class ConvExercise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvExercise, self).__init__()\n",
    "        \n",
    "        #(1,28,28) -> (12, 22, 25)\n",
    "        self.conv1 = nn.Conv2d(1, 12, (9,6), padding=1)\n",
    "        \n",
    "        #(12, 22, 25) -> (20, 8, 7)\n",
    "        self.conv2 = nn.Conv2d(12, 20, (5,9), stride=3, padding=(2,1))\n",
    "        \n",
    "        #(20, 8, 7) -> (25, 8, 7)\n",
    "        self.conv3 = nn.Conv2d(20, 25, (5,7), padding=(2,3))\n",
    "        \n",
    "        #(25, 8, 7) -> (30, 4, 4)\n",
    "        self.conv4 = nn.Conv2d(25, 30, (5,4))\n",
    "        \n",
    "        #480 -> 128\n",
    "        self.linear1 = nn.Linear(480, 128)\n",
    "        \n",
    "        #128 -> 10\n",
    "        self.linear2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #(1,28,28) -> (12, 22, 25)\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        \n",
    "        #(12, 22, 25) -> (20, 8, 7)\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        \n",
    "        #(20, 8, 7) -> (25, 8, 7)\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        \n",
    "        #(25, 8, 7) -> (30, 4, 4)\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        \n",
    "        #(30, 4, 4) -> 480\n",
    "        x = x.view(-1, 480)\n",
    "        \n",
    "        #480 -> 128\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        \n",
    "        #128 -> 10\n",
    "        x = F.softmax(self.linear2(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = ConvExercise()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convolutional Neural Network with Max Pooling\n",
    "'''\n",
    "class ConvNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet3, self).__init__()\n",
    "        \n",
    "        #(1,28,28) -> (16,24,24)\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        \n",
    "        #(16,12,12) -> (32,10,10)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        \n",
    "        #(32,5,5) -> (64,3,3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        \n",
    "        #(64,3,3) -> (64,3,3)\n",
    "        self.dropout = nn.Dropout2d(.2)\n",
    "        \n",
    "        #(64,3,3) -> (128,2,2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 2)\n",
    "        \n",
    "        #512 -> 10\n",
    "        self.linear = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #(1,28,28) -> (16,24,24)\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        \n",
    "        #(16,24,24) -> (16,12,12)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        \n",
    "        #(16,12,12) -> (32,10,10)\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        \n",
    "        #(32,10,10) -> (32,5,5)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        \n",
    "        #(32,5,5) -> (64,3,3)\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        \n",
    "        #(64,3,3) -> (64,3,3)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #(64,3,3) -> (128,2,2)\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        \n",
    "        #(128,2,2) -> 512\n",
    "        x = x.view(-1, 512)\n",
    "        \n",
    "        #512 -> 10\n",
    "        x = F.softmax(self.linear(x), dim=1)\n",
    "        \n",
    "        return x        \n",
    "    \n",
    "model = ConvNet3()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convolutional Neural Network with Max Pooling and new activation functions\n",
    "'''\n",
    "class ConvNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet4, self).__init__()\n",
    "        #(1,28,28) -> (16,24,24)\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        \n",
    "        #(16,12,12) -> (32,10,10)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        \n",
    "        #(32,5,5) -> (64,3,3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        \n",
    "        #(64,3,3) -> (64,3,3)\n",
    "        self.dropout = nn.Dropout2d(.2)\n",
    "        \n",
    "        #(64,3,3) -> (128,2,2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 2)\n",
    "        \n",
    "        #(128,2,2) -> (128,2,2)\n",
    "        self.prelu = nn.PReLU()\n",
    "        \n",
    "        #512 -> 10\n",
    "        self.linear = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #(1,28,28) -> (16,24,24)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        #(16,24,24) -> (16,12,12)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        \n",
    "        #(16,12,12) -> (32,10,10)\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        \n",
    "        #(32,10,10) -> (32,5,5)\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        \n",
    "        #(32,5,5) -> (64,3,3)\n",
    "        x = F.leaky_relu(self.conv3(x), .0001)\n",
    "        \n",
    "        #(64,3,3) -> (64,3,3)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #(64,3,3) -> (128,2,2)\n",
    "        x = self.prelu(self.conv4(x))\n",
    "        \n",
    "        #(128,2,2) -> 512\n",
    "        x = x.view(-1, 512)\n",
    "        \n",
    "        #512 -> 10\n",
    "        x = F.softmax(self.linear(x), dim=1)\n",
    "        \n",
    "        return x        \n",
    "    \n",
    "model = ConvNet4()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
